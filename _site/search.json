[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a email marketer with a background in statistical programming and a passion for R, data visualization, Shiny and machine learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "markvanderstay.com",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nViewing Twitter Influence With Network Graphs\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis Of Twitter Data With R\n\n\n7 min\n\n\n\nTwitter\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\nMar 28, 2023\n\n\n\n\n\n\n\n\n\n\n\nLDA With R Makes Twitter Data Insights Easy\n\n\n15 min\n\n\n\nTwitter\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\nMar 25, 2023\n\n\n\n\n\n\n\n\n\n\n\nUsing R for Twitter Analysis\n\n\n6 min\n\n\n\nTwitter\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\nMar 21, 2023\n\n\n\n\n\n\n\n\n\n\n\nAnalysing GA4 Data With R\n\n\n7 min\n\n\n\nGA4\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index1.html",
    "href": "posts/welcome/index1.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/using-google-analytics-ga4-data-in-r/index.html",
    "href": "posts/using-google-analytics-ga4-data-in-r/index.html",
    "title": "Analysing GA4 Data With R",
    "section": "",
    "text": "In marketing there’s often a gap between the collection and reporting of data. That gap is Analysis.\nFor small businesses without in-house expertise the analysis of marketing data is a giant void. GA4 makes it easy to collect marketing data but it’s much harder – and more expensive – to report it thoroughly and check for meaning.\nSearching out meaning in data inevitably falls by the wayside in small businesses. Know why?\n‘Too busy’ you say?\nWell yes business owners face myriad tasks day-to-day. But how about this…\nThere’s a steep learning curve. Yep, the clicks and sales keep coming so why bother trying to understand all that data?\nJust keep going and hand over the collection and analysis to someone else. Right?\nWRONG!\nIf you don’t know what you’re looking at you can’t tell if you’re being sold a dud.\nThat ‘someone else’ you’re paying for is a bit of a lottery.\nHiring a third party doesn’t mean the task of analysing your data will be done correctly. I’m going to tell you a story.\nIt’s a simple one and it demonstrates this issue nicely.\nOur central character is Phil (and no, that’s not his real name).\nPhil showed me the report he’d received from his outsourced SEO expert. In it the word ‘improvement’ featured in nearly every paragraph.\nImprovement this, improvement that. Poor Phil.\nHe thought he was doing great. Was he?\nIt was impossible to tell if the expert’s language used was simply to hide the truth. Nevertheless, Phil never stopped to consider if improvement really meant that.\nYou know why?\nYes, yes, he was busy. Sure.\nBut worse than that.\nOK, I’ll tell you.\nThere was not the slightest consideration of business objectives in Phil’s entire strategy. So the whole report really could have been showing improvements.\nBut we’d never know what had improved.\nI took a look at Phil’s stats and I’ll tell you about that later.\nA month on month increase in page views is not an improvement unless your business objective is page views.\nAn increase in site bounces is not an improvement. Plain as day. It’s an increase. Bounces going up? Poor Phil.\nThe saddest part though…\nPhil is no exception.\nMost clients seem stumped when I ask them what the goal of analysing their data is.\nAsk that question to a new client. See the look of surprise and bewilderment? No-one has asked them before. Why would they? Isn’t it obvious? Higher revenues! More clicks! \nIt depends.\n\n\nYou know the saying ‘if you only have a hammer everything looks like a nail’ ?\nSometimes progress is better examined by looking at metrics not reported in the standard GA4 interface.\nEven those owners who can state their objectives will not have considered that.\nHere’s an example.\nPerhaps it would be more useful to check what itemsets customers put in their basket. Then we can ask new questions:\n\nWas there something driving their choices?\nCan we predict what items they’re likely to choose next?\nShould we look at social data to examine sentiment?\n\nThere are many factors at play so why limit ourselves to what we’re served by Google?\nI’ll demonstrate below how to retrieve Google Analytics (GA) data using the GA4 API directly in R. We’ll be using the excellent googleAnalyticsR library by Mark Edmondson and in future articles I’ll go further with some analysis techniques but all will flow from this initial, fundamental step.\nBefore we begin plugging away with R there’s some setup necessary in Google Cloud."
  },
  {
    "objectID": "index.html#get-credentials-in-google-cloud-console",
    "href": "index.html#get-credentials-in-google-cloud-console",
    "title": "markvanderstay.com",
    "section": "Get Credentials in Google Cloud Console",
    "text": "Get Credentials in Google Cloud Console\nWe’ll need to enable some APIs so first head on over to Google Cloud Console and create a new project.\nIn this new project enable the Analytics Reporting API, Google Analytics API and the Google Analytics Data API. Yep, all three. Not all of these will be necessary straight away but you’ll need them soon enough so may as well do this now.\nStill in the cloud console, create a new OAuth client ID in order to allow login and retrieval of the data while in R. This will be a desktop app so ;\n\nClick Credentials\nCreate Credentials\nChoose OAuth client ID\nSelect ‘Desktop app’ and give it a nice name. Something catchy.\nClick Create\n\nNow, back at the credentials page, download your credentials as a .json file.\n\nClick the credentials you just created\nClick ‘Download JSON’\n\n\n\nOnce you've setup your credentials, download them as JSON\n\n\n\nOnce that’s all setup we’re done with Google. Fire up RStudio if you haven’t already."
  },
  {
    "objectID": "index.html#setup-a-new-project-in-rstudio",
    "href": "index.html#setup-a-new-project-in-rstudio",
    "title": "markvanderstay.com",
    "section": "Setup A New Project in RStudio",
    "text": "Setup A New Project in RStudio\nNow there’s obviously many ways to set up a project but I’m fastidious when it comes to keeping everything atomic, preferring to keep each project in its own directory. This way, I can keep my .Renviron file unique to a project and should I need to move the project somewhere, I can just move the folder and it’s done. Enough about me.\nCreate a new folder for this project and set it the working directory and create a .Renviron file with the following contents…\nGAR_CLIENT_JSON=./client.json\n…and move the `client.json` file downloaded from Google into your project folder.\nObligatory package install. We’ll be using the excellent `googleAnalyticsR`.\ninstall.packages(\"googleAnalyticsR\")\nCreate a new R script file and load the library we just installed.\nlibrary(googleAnalyticsR)\nYou’ll notice a message in the console similar to the following…\n Setting client.id from ./client.jso\n No consent file found\nNo bother, we’ll then authorise with…\nga_auth()\nYou’ll be prompted to choose with identity to login with and you simply select an option in the console window. So far, so good.\nIf you wish to see the list of accounts you have access to you’ll have to use the following to return a character array fo account names;\naccount_list <- ga_account_list()\nRemember, GA4 doesn’t use views like Universal Analytics does. Just stick with using propertyID and you’ll be fine."
  },
  {
    "objectID": "index.html#reading-ga4-data-with-r",
    "href": "index.html#reading-ga4-data-with-r",
    "title": "markvanderstay.com",
    "section": "Reading GA4 Data With R",
    "text": "Reading GA4 Data With R\nLet’s see some data, shall we? With the property ID you’re interested in you simply use;\nmetadata <- ga_meta(\"data\", propertyId = XXXXXXXXX)\n\nDimensions, Metrics and Filters\nLet’s check out a few GA4 dimensions and see what happens.\n dimensions <- ga_data(\n    XXXXXXXXX,\n    metrics = c(\"activeUsers\",\"sessions\"),\n    dimensions = c(\"date\",\"city\",\"dayOfWeek\"),\n    date_range = c(\"2023-01-01\", \"2023-01-31\")\n )\nYou’ll notice that GA4 is returning a maximum of 100 rows. Should you wish to see all rows you can use the `limit` parameter as follows;\n dimensions <- ga_data(\n    XXXXXXXXX,\n    metrics = c(\"activeUsers\",\"sessions\"),\n    dimensions = c(\"date\",\"city\",\"dayOfWeek\"),\n    date_range = c(\"2023-01-01\", \"2023-01-31\"),\n    limit = -1\n )\neventNames are used as they appear when using the GA4 web interface. But if you’re in any doubt you can always check…\n events <- ga_data(\n    XXXXXXXXX,\n    metrics = NULL,\n    dimensions = \"eventName\",\n    date_range =c(\"2023-01-01\", \"2023-01-31\")\n )\nTaking this further, we can filter events to just see those of interest. For example, I’ve setup a custom event that is fired when a Klaviyo form is submitted. We’ll use the `ga_data_filter` function like so;\nklaviyo <- ga_data_filter(eventName==\"klaviyo_form_submission\")\nAnd then use the filter it creates in a query;\n filteredKlaviyo <- ga_data(\n    XXXXXXXXX,\n    metrics = \"eventCount\",\n    dimensions = \"eventName\",\n    dim_filters = klaviyo,\n    date_range =c(\"2023-01-01\", \"2023-01-31\")\n )\nThe results of this query are basic because this filter isn’t particularly useful, it’s just a demo.\n\n\n\nGA4 data with filtering for custom event named ‘klaviyo_form_submission’.\n\n\nLastly, let’s check out the realtime data. Handy for taking snapshots of running campaigns…\n ga_data(\n    propertyId = XXXXXXXXX, \n    raw_json = '{\"metrics\":[{\"name\":\"activeUsers\"}],\"limit\":100,\"returnPropertyQuota\":true}',\n    realtime = TRUE)\nThere you have it. All of the above is enough to get you started examining marketing data in R. You can then cut and slice it however you prefer. Maybe you’ll use ggplot2 for some funky dataviz or merge with offline data ready for import back into GA4.\nEnjoy!"
  },
  {
    "objectID": "posts/using-google-analytics-ga4-data-in-r/index.html#get-credentials-in-google-cloud-console",
    "href": "posts/using-google-analytics-ga4-data-in-r/index.html#get-credentials-in-google-cloud-console",
    "title": "Analysing GA4 Data With R",
    "section": "Get Credentials in Google Cloud Console",
    "text": "Get Credentials in Google Cloud Console\nWe’ll need to enable some APIs so first head on over to Google Cloud Console and create a new project.\nIn this new project enable the Analytics Reporting API, Google Analytics API and the Google Analytics Data API. Yep, all three. Not all of these will be necessary straight away but you’ll need them soon enough so may as well do this now.\nStill in the cloud console, create a new OAuth client ID in order to allow login and retrieval of the data while in R. This will be a desktop app so ;\n\nClick Credentials\nCreate Credentials\nChoose OAuth client ID\nSelect ‘Desktop app’ and give it a nice name. Something catchy.\nClick Create\n\nNow, back at the credentials page, download your credentials as a .json file.\n\nClick the credentials you just created\nClick ‘Download JSON’\n\n\n\nOnce you’ve setup your credentials, download them as JSON\n\n\n\nOnce that’s all setup we’re done with Google. Fire up RStudio if you haven’t already."
  },
  {
    "objectID": "posts/using-google-analytics-ga4-data-in-r/index.html#setup-a-new-project-in-rstudio",
    "href": "posts/using-google-analytics-ga4-data-in-r/index.html#setup-a-new-project-in-rstudio",
    "title": "Analysing GA4 Data With R",
    "section": "Setup A New Project in RStudio",
    "text": "Setup A New Project in RStudio\nNow there’s obviously many ways to set up a project but I’m fastidious when it comes to keeping everything atomic, preferring to keep each project in its own directory. This way, I can keep my .Renviron file unique to a project and should I need to move the project somewhere, I can just move the folder and it’s done. Enough about me.\nCreate a new folder for this project and set it the working directory and create a .Renviron file with the following contents…\nGAR_CLIENT_JSON=./client.json\n…and move the `client.json` file downloaded from Google into your project folder.\nObligatory package install. We’ll be using the excellent `googleAnalyticsR`.\ninstall.packages(\"googleAnalyticsR\")\nCreate a new R script file and load the library we just installed.\nlibrary(googleAnalyticsR)\nYou’ll notice a message in the console similar to the following…\n Setting client.id from ./client.jso\n No consent file found\nNo bother, we’ll then authorise with…\nga_auth()\nYou’ll be prompted to choose with identity to login with and you simply select an option in the console window. So far, so good.\nIf you wish to see the list of accounts you have access to you’ll have to use the following to return a character array fo account names;\naccount_list <- ga_account_list()\nRemember, GA4 doesn’t use views like Universal Analytics does. Just stick with using propertyID and you’ll be fine."
  },
  {
    "objectID": "posts/using-google-analytics-ga4-data-in-r/index.html#reading-ga4-data-with-r",
    "href": "posts/using-google-analytics-ga4-data-in-r/index.html#reading-ga4-data-with-r",
    "title": "Analysing GA4 Data With R",
    "section": "Reading GA4 Data With R",
    "text": "Reading GA4 Data With R\nLet’s see some data, shall we? With the property ID you’re interested in you simply use;\nmetadata <- ga_meta(\"data\", propertyId = XXXXXXXXX)\n\nDimensions, Metrics and Filters\nLet’s check out a few GA4 dimensions and see what happens.\n dimensions <- ga_data(\n    XXXXXXXXX,\n    metrics = c(\"activeUsers\",\"sessions\"),\n    dimensions = c(\"date\",\"city\",\"dayOfWeek\"),\n    date_range = c(\"2023-01-01\", \"2023-01-31\")\n )\nYou’ll notice that GA4 is returning a maximum of 100 rows. Should you wish to see all rows you can use the `limit` parameter as follows;\n dimensions <- ga_data(\n    XXXXXXXXX,\n    metrics = c(\"activeUsers\",\"sessions\"),\n    dimensions = c(\"date\",\"city\",\"dayOfWeek\"),\n    date_range = c(\"2023-01-01\", \"2023-01-31\"),\n    limit = -1\n )\neventNames are used as they appear when using the GA4 web interface. But if you’re in any doubt you can always check…\n events <- ga_data(\n    XXXXXXXXX,\n    metrics = NULL,\n    dimensions = \"eventName\",\n    date_range =c(\"2023-01-01\", \"2023-01-31\")\n )\nTaking this further, we can filter events to just see those of interest. For example, I’ve setup a custom event that is fired when a Klaviyo form is submitted. We’ll use the `ga_data_filter` function like so;\nklaviyo <- ga_data_filter(eventName==\"klaviyo_form_submission\")\nAnd then use the filter it creates in a query;\n filteredKlaviyo <- ga_data(\n    XXXXXXXXX,\n    metrics = \"eventCount\",\n    dimensions = \"eventName\",\n    dim_filters = klaviyo,\n    date_range =c(\"2023-01-01\", \"2023-01-31\")\n )\nThe results of this query are basic because this filter isn’t particularly useful, it’s just a demo.\n\n\n\nGA4 data with filtering for custom event named ‘klaviyo_form_submission’.\n\n\nLastly, let’s check out the realtime data. Handy for taking snapshots of running campaigns…\n ga_data(\n    propertyId = XXXXXXXXX, \n    raw_json = '{\"metrics\":[{\"name\":\"activeUsers\"}],\"limit\":100,\"returnPropertyQuota\":true}',\n    realtime = TRUE)\nThere you have it. All of the above is enough to get you started examining marketing data in R. You can then cut and slice it however you prefer. Maybe you’ll use ggplot2 for some funky dataviz or merge with offline data ready for import back into GA4.\nEnjoy!"
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis/index.html",
    "href": "posts/using-r-for-twitter-analysis/index.html",
    "title": "Using R for Twitter Analysis",
    "section": "",
    "text": "Social media has emerged as a prominent tool for expressing opinions, sharing news, and engaging in discussions. Twitter, with over 330 million active users worldwide, has become an indispensable tool for businesses, governments, and individuals to promote their messages and campaigns.\nData analysis of Twitter data is becoming increasingly important, as it provides valuable insights into the patterns and trends of Twitter users. Using the R programming language, it is possible to clean and analyze tweet data, and then create visualizations that help to illustrate the patterns and trends that emerge.\nIn this blog post, I will demonstrate how to use R to extract and clean tweets, create a corpus, and plot the results. By the end you will have a better understanding of how Twitter data analysis can be used for advertising or awareness campaigns on any topic.\nThe R programming language is a powerful tool for data analysis and visualization and it is especially useful for analyzing Twitter data. Whether you are interested in analysing Twitter data for personal or professional purposes, the techniques I’ll demonstrate here and in the following posts will provide you with a solid foundation for getting started with Twitter data analysis using R."
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis/index.html#setup-authorise-rtweet",
    "href": "posts/using-r-for-twitter-analysis/index.html#setup-authorise-rtweet",
    "title": "Using R for Twitter Analysis",
    "section": "1. Setup & authorise rtweet",
    "text": "1. Setup & authorise rtweet\nLet’s begin with the usual library statements. We’ll need a few for this post so may as well get them all at once.\nlibrary(rtweet)\nlibrary(httr)\nlibrary(dplyr)\nlibrary(qdap)\nlibrary(qdapRegex)\nlibrary(tm)\nlibrary(ggplot2)\nNext up, authorise the rtweet package to use your Twitter account. It gets your details from the browser and you should only need to do this once. The authors provide a useful vignette. Check it out by running vignette(\"auth\", \"rtweet\") in the console.\n#authorise\nauth_setup_default()"
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis/index.html#extract-tweets",
    "href": "posts/using-r-for-twitter-analysis/index.html#extract-tweets",
    "title": "Using R for Twitter Analysis",
    "section": "2. Extract tweets",
    "text": "2. Extract tweets\nFirst off, I’ll use search_tweets to extract a sample of Twitter data. In this example I’ll use tweets containing the hashtag ‘#mentalhealth’. Mental health is becoming an increasingly important issue and there will be lots of tweets with this tag.\nI’ll exclude retweets and filter for those in English only.\n#get 2000 tweets with the #mentalhealth hashtag\ntwtr <- search_tweets(\"#mentalhealth\",\n                       n = 1000,\n                       include_rts = FALSE,\n                       lang = \"en\")"
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis/index.html#clean-data-and-creating-a-corpus.",
    "href": "posts/using-r-for-twitter-analysis/index.html#clean-data-and-creating-a-corpus.",
    "title": "Using R for Twitter Analysis",
    "section": "3. Clean data and creating a corpus.",
    "text": "3. Clean data and creating a corpus.\nChecking the text of the tweets inside our shiny, new data frame reveals that we need to do some tidying of the data.\n#check tweet text\ntweet_text <- twtr$text\nhead(tweet_text)\nThere’s a lot of emojis, URLs, punctuation and other stuff going on here.\n[1] \"Start your day right with a quote from SUGA of BTS \\n\\n#MindsetApp #Quotes #Motivation #Wellness #Positivity #SelfCare #MentalHealth #Kpop #BTS #SUGA https://t.co/GXglBR7rrk\"\n[2] \"Remember: There is no health without mental health! #MentalHealth is just as important as physical health, and asking for help is a normal part of life. You should never feel like you have to take on the world  alone. #NationalFlowerDay #FlowerDay https://t.co/4SEVDf4c4J\"\n[3] \"Everyone either knows someone struggling with their #MentalHealth or they do themselves. \\n\\nSome things should transcend politics, that’s what @TedLasso is trying to tell us.\\n\\nI admire anyone who can ask for help. \\n https://t.co/Iz32fcP3xa\"\n[4] \"@Iamdepr47974144 Have you talked to anyone about how you feel? #mentalhealth #depression\"\n[5] \"#postivevibes #quotes #peace #innerchild #mentalhealth #love #motto #inspiration #health \\n\\n Blissful Cleaning LLC https://t.co/VyfA8yE8IU\" \nI’ll use the tm package to help with this but first I can use a simple regular expression. I’ll remove URLs and special characters such as emojis by simply keeping only letters.\n# Remove special characters\ntweet_text_az <- gsub(\"[^A-Za-z]\", \" \", tweet_text)\nBefore I proceed I’ll convert the text I extracted into a corpus so that I can use the tm package for text mining. Creating a corpus is important because it provides a structured way to work with text data and enables more efficient analysis.\nAnd look how easy it is…\n# Create a corpus (transform tweets into documents)\ntweet_corpus <- tweet_text_az %>%\n                VectorSource() %>%\n                Corpus()"
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis/index.html#usage-of-the-tm-package-for-text-mining",
    "href": "posts/using-r-for-twitter-analysis/index.html#usage-of-the-tm-package-for-text-mining",
    "title": "Using R for Twitter Analysis",
    "section": "4. Usage of the tm package for text mining",
    "text": "4. Usage of the tm package for text mining\nNow that’s done I’ll use a few tm package functions to do a bit more cleaning.\n# Remove stop words, extra whitespace and transform to lower case\ntweet_corpus_no_stop <- tm_map(tweet_corpus, removeWords, stopwords(\"english\"))\ntweet_corpus_lower <- tm_map(tweet_corpus_no_stop, tolower)\ntweet_corpus_docs <- tm_map(tweet_corpus_lower, stripWhitespace)\nAfter a quick check of word frequency I’ll remove words that don’t serve a purpose.\n# remove custom stops and check frequency again\n# 1. remove custom stops\ncustom_stop <- c('mentalhealth', 'health', 'mental', 's', 'i', 'can', 'amp',\n                'day', 'the', 'mentalhealthawareness', 't', 'it',\n                'internationaldayofhappiness', 'will', 'a', 'get', 'you',\n                'need', 'take', 'new', 'one', 'make', 're', 'for', 'march',\n                'm', 'if', 'to', 'via', 'don', 'just', 'th', 'may', 'way')\n\ntweet_corpus_cust <- tm_map(tweet_corpus_docs, removeWords, custom_stop)\n\n# 2. check frequency\nword_freq_cust <- freq_terms(tweet_corpus_cust, 25) \nword_freq_cust\nWhat I’m left with is a list of words I can use to help promote mental health awareness. The end result is not particularly suprising given the subject matter but it serves as a good example of the possibilities of using Twitter data for other campaigns such as brand management.\nI’ll quickly put together a bar chart showing word vs freq in ggplot.\n\n# visualise the most popular terms (those with a frequency>100)\nword_freq_100 <- subset(word_freq_cust, FREQ >= 100)\n\nggplot(word_freq_100, aes(x = reorder(WORD, -FREQ), y = FREQ)) +\n       geom_bar(stat= 'identity', fill = 'midnightblue') +\n       xlab('Word') +\n       ylab('Frequency') +\n       ggtitle('Word Frequency', subtitle = '#mentalhealth') +\n       theme(\n         axis.text.x = element_text(angle = 90, hjust = 1)\n       )\n\n\n\nAfter cleaning Twitter data: Terms with frequency > 100 visualised."
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis/index.html#conclusion",
    "href": "posts/using-r-for-twitter-analysis/index.html#conclusion",
    "title": "Using R for Twitter Analysis",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nIn this blog post, I demonstrated how R can be used to extract and clean tweets, create a corpus and analyze Twitter data which potentially offers valuable insights into the behaviour and trends of Twitter users. These insights could prove beneficial for personal or professional purposes, such as advertising or awareness campaigns.\nIn following posts I’ll take this process further. What awaits is topic modelling, sentiment analysis and social graph visualisations."
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis-lda/index.html",
    "href": "posts/using-r-for-twitter-analysis-lda/index.html",
    "title": "LDA With R Makes Twitter Data Insights Easy",
    "section": "",
    "text": "Topic modeling is a statistical technique utilised to identify themes or topics within a vast collection of texts. One of the most popular algorithms used for topic modeling is Latent Dirichlet Allocation (LDA) which is used to uncover the underlying topics present in a ‘corpus’ of documents.\nLDA helps analysts to understand which subjects are being discussed in text and to group similar documents together. This makes it easier to see patterns and trends in your data, understand the main themes of textual data, and make better decisions based on the information available.\nIn this post I’ll provide a demonstration of Latent Dirichlet Allocation on Twitter data. I’ll focus on three main reasons for using LDA;\n\nData reduction. LDA can group related documents into topics By reducing the complexity of the data by it can be easier to analyse and identify patterns.\nTopic identification. LDA can be used to identify the most important topics within a large corpus of documents. This can provide insight into the topics that are most frequently discussed.\nImproved understanding. LDA can help to gain a deeper understanding of the content within your data.\n\n\n\nI begin as before with extracting a sample of tweets…\nlibrary(rtweet)\nlibrary(httr)\nlibrary(dplyr)\nlibrary(qdap)\nlibrary(qdapRegex)\nlibrary(tm)\n\n\n# authorise\nauth_setup_default()\n\n\n# get 2000 tweets with our required hashtag\ntwtr <- search_tweets(\"parliament\",\n                       n = 2000,\n                       include_rts = FALSE,\n                       lang = \"en\")\n… then keep only the text data before tidying. Removing special characters and URLs will remove some tweets altogether but that’s partly the point.\n# create data frame of tweet text\ntweet_text <- twtr$text\n\n# remove special characters\ntweet_text_no_url <- rm_twitter_url(tweet_text)\ntweet_text_az <- gsub(\"[^A-Za-z]\", \" \", tweet_text_no_url)\nhead(tweet_text_az)\nThen it’s time to make the corpus (where tweets become documents) and tidy some more with the tm package.\n# create a corpus (transform tweets into documents)\ntweet_corpus <- tweet_text_az %>%\n                VectorSource() %>%\n                Corpus()\n\n# remove stop words, extra whitespace and transform to lower case\ntweet_corpus_no_stop <- tm_map(tweet_corpus, removeWords, stopwords(\"english\"))\ntweet_corpus_lower <- tm_map(tweet_corpus_no_stop, tolower)\ntweet_corpus_docs <- tm_map(tweet_corpus_lower, stripWhitespace)\nBefore beginning my analysis I’ll have to create the Document Term Matrix from the corpus.\n# create a DTM\ndtm <- DocumentTermMatrix(tweet_corpus_docs)\nChecking out the details of the DTM I see that I have 2000 documents with >8800 terms. Several rows of the documents are shown below with the terms that appear in each document. Note there’s a lot of zeros because some terms are found only rarely. This is also why ‘sparcity’ is 100%.\n<<DocumentTermMatrix (documents: 2000, terms: 8825)>>\nNon-/sparse entries: 33734/17616266\nSparsity           : 100%\nMaximal term length: 38\nWeighting          : term frequency (tf)\nSample             :\n \n     Terms\nDocs   amp boris can ireland johnson northern parliament people the will\n1063   0     0   0       0       0        0          2      0   1    0\n1119   0     0   0       0       1        0          1      0   2    0\n1169   0     1   0       0       0        0          2      0   1    0\n1223   0     0   0       0       0        0          1      0   1    0\n1471   0     0   0       0       0        0          1      0   1    0\n1550   0     0   0       0       0        0          1      0   0    0\n1769   0     0   0       0       0        0          1      1   1    0\n533    0     0   1       0       0        0          1      0   1    0\n917    0     0   0       0       2        0          1      1   1    0\n978    1     0   0       0       0        0          1      1   1    0 ...\nRows (documents) that don’t have any terms – where the terms columns are all zeros – can be removed. Those documents won’t tell us anything anyway.\nSumming the word counts in each row is straighforward with apply.\n# sum word counts in each document\nrow_terms <- apply(dtm , 1, sum)\n\n# keep rows where sum is > 0\ntweet_dtm <- dtm[row_terms > 0, ]\n\n\n\nHere’s the fun part. Running the LDA is simple. I just need to decide how many topics I want to start with. Usually I choose an odd number. I’ve no idea why I do it this way. I’ll start with five.\n# the lda function is within the topicmodels package\nlibrary(topicmodels)\n\ntweet_lda5 <- LDA(tweet_dtm, 5)\nAnd inspect the results. I should have five topics and be able to view the top terms under each of them using the terms() function.\n\n# and check the results\nterms_lda5 <- terms(tweet_lda5, 15)\nterms_lda5\n       Topic 1         Topic 2      Topic 3      Topic 4      Topic 5     \n[1,] \"parliament\"    \"parliament\" \"parliament\" \"parliament\" \"parliament\"\n[2,] \"the\"           \"the\"        \"party\"      \"johnson\"    \"northern\"  \n[3,] \"opposition\"    \"right\"      \"amp\"        \"boris\"      \"ireland\"   \n[4,] \"government\"    \"people\"     \"will\"       \"the\"        \"that\"      \n[5,] \"can\"           \"can\"        \"people\"     \"misled\"     \"will\"      \n[6,] \"amp\"           \"will\"       \"boris\"      \"committee\"  \"like\"      \n[7,] \"get\"           \"now\"        \"gave\"       \"lied\"       \"the\"       \n[8,] \"bill\"          \"just\"       \"get\"        \"rules\"      \"there\"     \n[9,] \"mps\"           \"rahul\"      \"vote\"       \"misleading\" \"great\"     \n[10,] \"petition\"      \"act\"        \"gbnews\"     \"mislead\"    \"yes\"       \n[11,] \"anti\"          \"member\"     \"not\"        \"partygate\"  \"democracy\" \n[12,] \"people\"        \"amp\"        \"blair\"      \"defence\"    \"say\"       \n[13,] \"will\"          \"european\"   \"job\"        \"didn\"       \"always\"    \n[14,] \"they\"          \"crisis\"     \"you\"        \"evidence\"   \"majority\"  \n[15,] \"homosexuality\" \"session\"    \"make\"       \"lying\"      \"says\" \n\n\n\nIt’s possible to see that there’s overlap between the topics but there are five1 as requested. Let me take a stab at naming each;\n\n1: Parliament’s latest discussion of a bill related to homosexuality.\n2: A more general topic. Possibly a mix of topics involving Europe & Rahul Gandhi.\n3: Discussion on GBNews about Tony Blair’s latest comments regarding Boris Johnson.\n4: Partygate. Without a doubt this topic centres on BoJo, his actions and the resulting enquiry.\n5: Northern Ireland, possibly discussing issues surrounding the region, such as its relationship with the UK and Brexit.\n\nLDA is a fairly standard method of topic modelling and as shown above it’s not perfect. It is up to the analyst to decide how many topics to create, then to decide how much the topics overlap. Finding topics that do not overlap or repeat can be difficult.\nAt this point one might take a moment to wonder if it’s worthwhile creating a visualisation like a cluster plot or dendogram. I advise against it. Topic modelling is based on word counts and some words will appear in multiple documents as seen above. Clustering on the other hand is based on the distance between objects and each object belongs in a single cluster.\n\n\n\nRemember, LDA assumes that each document in the corpus is a mixture of a number of topics and that each word in a document is related to one of those topics. With Bayesian inference, LDA identifies the most probable set of topics for each document and the most probable set of words for each topic.\nThe resulting output is a set of topics represented by a group of words with associated probabilities and a topic distribution for each document represented by a probability distribution over the set of topics. OK?\nI’ll plot those probabilities next.\nI’ll use the same data above but this time demonstrate a few more options available in LDA(). The output can then be passed to tidy() and finally to ggplot.\nlibrary(tidytext)\n\ntweet_lda_gibbs <- LDA(tweet_dtm,\n                       k = 5,\n                       method = \"Gibbs\",\n                       control = list(seed = 33)\n                   ) %>%\n                   tidytext::tidy(matrix = \"beta\")\nNote: In the snippet above it’s necessary to use tidytext::tidy in full possibly due to a bug in the topicmodels library.\nNow I’ll arrange the word probabilities by topic and transform the terms into factors. Factors are necessary because one single word can appear in many topics. For example, the word ‘parliament’ can be used in each of the topics shown above and have a different context in each.\nlibrary(tidyverse)\n\ntweet_word_probs <- tweet_lda_gibbs %>%\n                    group_by(topic) %>%\n                    top_n(15, beta) %>%\n                    ungroup() %>%\n                    mutate(term_n = fct_reorder(term, beta))  \nAnd finally…\n\n#plot word probabilities\nggplot(tweet_word_probs,\n       aes(term_n,\n           beta,\n           fill = as.factor(topic)\n       )\n) +\ngeom_col(show.legend = FALSE) +\n         coord_flip() +\n         facet_wrap(~ topic, scales = \"free\")\nThe resulting image shows much more clearly that the topics involve not just the UK parliament and BoJo’s (alleged) breach of the rules but also the Ugandan parliament. Topics 3 and 5 are possibly the same topic as they concern Kasha Jacqueline Nabagesera and the Ugandan government’s criminalisation of LGBT rights. Creating a visualisation of word probabilities has helped to determine the quantity of topics. It’d be a good idea now to go back and refine the model.\n\n\n\n\nAs the world becomes increasingly data-driven, tools like Latent Dirichlet Allocation (LDA) have emerged as powerful instruments for extracting insights from large datasets. However, as with any tool, there are risks of misuse and misinterpretation. The similar topics in this post are a prime example.\nThe problem of similar topics in LDA analysis is not uncommon and it can be attributed to a variety of factors. In some cases it may be due to poor quality data with noise and irrelevant content obscuring the underlying patterns. Alternatively, an insufficient number of topics or homogeneity in the content being analysed can also cause LDA to group together different topics into one. Additionally, overfitting and slack pre-processing techniques can also lead to similar topics.\nThe consequences of similar topics in LDA analysis can be severe. If the results are used for decision-making purposes, the similarity can lead to incorrect conclusions and actions. It is crucial that data analysts exercise caution and thoroughly investigate the causes of similar topics.\nFurthermore, the LDA algorithm must not be used in isolation. It is imperative that researchers combine it with other techniques and knowledge to identify the underlying patterns that LDA cannot detect on its own.\nIn conclusion, LDA is a powerful tool, but its results are not infallible. A comprehensive approach to data analysis that includes multiple techniques and expert knowledge is necessary to ensure accurate and reliable results."
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis-lda/index.html#create-a-corpus-of-tweets",
    "href": "posts/using-r-for-twitter-analysis-lda/index.html#create-a-corpus-of-tweets",
    "title": "LDA With R Makes Twitter Data Insights Easy",
    "section": "1. Create a corpus of tweets",
    "text": "1. Create a corpus of tweets\nI begin as before with extracting a sample of tweets…\nlibrary(rtweet)\nlibrary(httr)\nlibrary(dplyr)\nlibrary(qdap)\nlibrary(qdapRegex)\nlibrary(tm)\n\n\n# authorise\nauth_setup_default()\n\n\n# get 2000 tweets with our required hashtag\ntwtr <- search_tweets(\"parliament\",\n                       n = 2000,\n                       include_rts = FALSE,\n                       lang = \"en\")\n… then keep only the text data before tidying. Removing special characters and URLs will remove some tweets altogether but that's partly the point.\n# create data frame of tweet text\ntweet_text <- twtr$text\n\n# remove special characters\ntweet_text_no_url <- rm_twitter_url(tweet_text)\ntweet_text_az <- gsub(\"[^A-Za-z]\", \" \", tweet_text_no_url)\nhead(tweet_text_az)\nThen it's time to make the corpus (where tweets become documents) and tidy some more with the tm package.\n# create a corpus (transform tweets into documents)\ntweet_corpus <- tweet_text_az %>%\n                VectorSource() %>%\n                Corpus()\n\n# remove stop words, extra whitespace and transform to lower case\ntweet_corpus_no_stop <- tm_map(tweet_corpus, removeWords, stopwords(\"english\"))\ntweet_corpus_lower <- tm_map(tweet_corpus_no_stop, tolower)\ntweet_corpus_docs <- tm_map(tweet_corpus_lower, stripWhitespace)\nBefore beginning my analysis I'll have to create the Document Term Matrix from the corpus.\n# create a DTM\ndtm <- DocumentTermMatrix(tweet_corpus_docs)\nChecking out the details of the DTM I see that I have 2000 documents with >8800 terms. Several rows of the documents are shown below with the terms that appear in each document. Note there's a lot of zeros because some terms are found only rarely. This is also why 'sparcity' is 100%.\n<<DocumentTermMatrix (documents: 2000, terms: 8825)>>\nNon-/sparse entries: 33734/17616266\nSparsity           : 100%\nMaximal term length: 38\nWeighting          : term frequency (tf)\nSample             :\n \n     Terms\nDocs   amp boris can ireland johnson northern parliament people the will\n1063   0     0   0       0       0        0          2      0   1    0\n1119   0     0   0       0       1        0          1      0   2    0\n1169   0     1   0       0       0        0          2      0   1    0\n1223   0     0   0       0       0        0          1      0   1    0\n1471   0     0   0       0       0        0          1      0   1    0\n1550   0     0   0       0       0        0          1      0   0    0\n1769   0     0   0       0       0        0          1      1   1    0\n533    0     0   1       0       0        0          1      0   1    0\n917    0     0   0       0       2        0          1      1   1    0\n978    1     0   0       0       0        0          1      1   1    0 ...\nRows (documents) that don't have any terms – where the terms columns are all zeros – can be removed. Those documents won't tell us anything anyway.\nSumming the word counts in each row is straighforward with apply.\n# sum word counts in each document\nrow_terms <- apply(dtm , 1, sum)\n\n# keep rows where sum is > 0\ntweet_dtm <- dtm[row_terms > 0, ]"
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis-lda/index.html#create-a-topic-model-with-lda",
    "href": "posts/using-r-for-twitter-analysis-lda/index.html#create-a-topic-model-with-lda",
    "title": "LDA With R Makes Twitter Data Insights Easy",
    "section": "2. Create a topic model with LDA()",
    "text": "2. Create a topic model with LDA()\nHere's the fun part. Running the LDA is simple. I just need to decide how many topics I want to start with. Usually I choose an odd number. I've no idea why I do it this way. I'll start with five.\n# the lda function is within the topicmodels package\nlibrary(topicmodels)\n\ntweet_lda5 <- LDA(tweet_dtm, 5)\nAnd inspect the results. I should have five topics and be able to view the top terms under each of them using the terms() function.\n\n# and check the results\nterms_lda5 <- terms(tweet_lda5, 15)\nterms_lda5\n       Topic 1         Topic 2      Topic 3      Topic 4      Topic 5     \n[1,] \"parliament\"    \"parliament\" \"parliament\" \"parliament\" \"parliament\"\n[2,] \"the\"           \"the\"        \"party\"      \"johnson\"    \"northern\"  \n[3,] \"opposition\"    \"right\"      \"amp\"        \"boris\"      \"ireland\"   \n[4,] \"government\"    \"people\"     \"will\"       \"the\"        \"that\"      \n[5,] \"can\"           \"can\"        \"people\"     \"misled\"     \"will\"      \n[6,] \"amp\"           \"will\"       \"boris\"      \"committee\"  \"like\"      \n[7,] \"get\"           \"now\"        \"gave\"       \"lied\"       \"the\"       \n[8,] \"bill\"          \"just\"       \"get\"        \"rules\"      \"there\"     \n[9,] \"mps\"           \"rahul\"      \"vote\"       \"misleading\" \"great\"     \n[10,] \"petition\"      \"act\"        \"gbnews\"     \"mislead\"    \"yes\"       \n[11,] \"anti\"          \"member\"     \"not\"        \"partygate\"  \"democracy\" \n[12,] \"people\"        \"amp\"        \"blair\"      \"defence\"    \"say\"       \n[13,] \"will\"          \"european\"   \"job\"        \"didn\"       \"always\"    \n[14,] \"they\"          \"crisis\"     \"you\"        \"evidence\"   \"majority\"  \n[15,] \"homosexuality\" \"session\"    \"make\"       \"lying\"      \"says\""
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis-lda/index.html#topic-naming-for-lda-data",
    "href": "posts/using-r-for-twitter-analysis-lda/index.html#topic-naming-for-lda-data",
    "title": "LDA With R Makes Twitter Data Insights Easy",
    "section": "3. Topic naming for LDA data",
    "text": "3. Topic naming for LDA data\nIt's possible to see that there's overlap between the topics but there are five1 as requested. Let me take a stab at naming each;\n\n1: Parliament's latest discussion of a bill related to homosexuality.\n2: A more general topic. Possibly a mix of topics involving Europe & Rahul Gandhi.\n3: Discussion on GBNews about Tony Blair's latest comments regarding Boris Johnson.\n4: Partygate. Without a doubt this topic centres on BoJo, his actions and the resulting enquiry.\n5: Northern Ireland, possibly discussing issues surrounding the region, such as its relationship with the UK and Brexit.\n\nLDA is a fairly standard method of topic modelling and as shown above it's not perfect. It is up to the analyst to decide how many topics to create, then to decide how much the topics overlap. Finding topics that do not overlap or repeat can be difficult.\nAt this point one might take a moment to wonder if it's worthwhile creating a visualisation like a cluster plot or dendogram. I advise against it. Topic modelling is based on word counts and some words will appear in multiple documents as seen above. Clustering on the other hand is based on the distance between objects and each object belongs in a single cluster."
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis-lda/index.html#plotting-word-probabilities",
    "href": "posts/using-r-for-twitter-analysis-lda/index.html#plotting-word-probabilities",
    "title": "LDA With R Makes Twitter Data Insights Easy",
    "section": "4. Plotting word probabilities",
    "text": "4. Plotting word probabilities\nRemember, LDA assumes that each document in the corpus is a mixture of a number of topics and that each word in a document is related to one of those topics. With Bayesian inference, LDA identifies the most probable set of topics for each document and the most probable set of words for each topic.\nThe resulting output is a set of topics represented by a group of words with associated probabilities and a topic distribution for each document represented by a probability distribution over the set of topics. OK?\nI'll plot those probabilities next.\nI'll use the same data above but this time demonstrate a few more options available in LDA(). The output can then be passed to tidy() and finally to ggplot.\nlibrary(tidytext)\n\ntweet_lda_gibbs <- LDA(tweet_dtm,\n                       k = 5,\n                       method = \"Gibbs\",\n                       control = list(seed = 33)\n                   ) %>%\n                   tidytext::tidy(matrix = \"beta\")\nNote: In the snippet above it's necessary to use tidytext::tidy in full possibly due to a bug in the topicmodels library.\nNow I'll arrange the word probabilities by topic and transform the terms into factors. Factors are necessary because one single word can appear in many topics. For example, the word 'parliament' can be used in each of the topics shown above and have a different context in each.\nlibrary(tidyverse)\n\ntweet_word_probs <- tweet_lda_gibbs %>%\n                    group_by(topic) %>%\n                    top_n(15, beta) %>%\n                    ungroup() %>%\n                    mutate(term_n = fct_reorder(term, beta))  \nAnd finally…\n\n#plot word probabilities\nggplot(tweet_word_probs,\n       aes(term_n,\n           beta,\n           fill = as.factor(topic)\n       )\n) +\ngeom_col(show.legend = FALSE) +\n         coord_flip() +\n         facet_wrap(~ topic, scales = \"free\")\nThe resulting image shows much more clearly that the topics involve not just the UK parliament and BoJo's (alleged) breach of the rules but also the Ugandan parliament. Topics 3 and 5 are possibly the same topic as they concern Kasha Jacqueline Nabagesera and the Ugandan government's criminalisation of LGBT rights. Creating a visualisation of word probabilities has helped to determine the quantity of topics. It'd be a good idea now to go back and refine the model."
  },
  {
    "objectID": "posts/using-r-for-twitter-analysis-lda/index.html#conclusion",
    "href": "posts/using-r-for-twitter-analysis-lda/index.html#conclusion",
    "title": "LDA With R Makes Twitter Data Insights Easy",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nAs the world becomes increasingly data-driven, tools like Latent Dirichlet Allocation (LDA) have emerged as powerful instruments for extracting insights from large datasets. However, as with any tool, there are risks of misuse and misinterpretation. The similar topics in this post are a prime example.\nThe problem of similar topics in LDA analysis is not uncommon and it can be attributed to a variety of factors. In some cases it may be due to poor quality data with noise and irrelevant content obscuring the underlying patterns. Alternatively, an insufficient number of topics or homogeneity in the content being analysed can also cause LDA to group together different topics into one. Additionally, overfitting and slack pre-processing techniques can also lead to similar topics.\nThe consequences of similar topics in LDA analysis can be severe. If the results are used for decision-making purposes, the similarity can lead to incorrect conclusions and actions. It is crucial that data analysts exercise caution and thoroughly investigate the causes of similar topics.\nFurthermore, the LDA algorithm must not be used in isolation. It is imperative that researchers combine it with other techniques and knowledge to identify the underlying patterns that LDA cannot detect on its own.\nIn conclusion, LDA is a powerful tool, but its results are not infallible. A comprehensive approach to data analysis that includes multiple techniques and expert knowledge is necessary to ensure accurate and reliable results.\n\nNotes:"
  },
  {
    "objectID": "posts/mastering-sentiment-analysis-of-twitter-data-with-r/index.html",
    "href": "posts/mastering-sentiment-analysis-of-twitter-data-with-r/index.html",
    "title": "Sentiment Analysis Of Twitter Data With R",
    "section": "",
    "text": "Following on from my last post outlining LDA, I’ll go further to analyse the sentiment of the tweet text. The most straightforward way to approach sentiment analysis is to use one of the many sentiment dictionaries available.\nI’m going to use the Loughran McDonald dictionary which has several sentiments defined. I’m choosing it simply out of familiarity and there are plenty of others to choose from. In this post I’ll subset only the positive and negative sentiments.\nHere’s refresher of the data I’ll be using. Remember this is 2000 tweets tagged ‘#parliament’ that I extracted from Twitter. The data is… messy.\n> print(twtr[1:10, \"text\"])\n\n# A tibble: 10 × 1\n   text                                                                                \n   <chr>                                                                               \n \n1 \"Parliament records accessed by @Iyervval prove your grandfather voted FOR the Emer…\n2 \"Shocking \\nShameful \\nDisgraceful \\n\\nPapers thrown at Speaker from Congress camp …\n3 \"To save one man, Modi ji is trampling the interests of 140 Cr people.\\n\\nTo protec…\n4 \"@QuetzalPhoenix \\\"vote\\\"?  What are the odds parliament &amp; the Scottish execu…\n5 \"RCN student members are at the Welsh Parliament today, looking forward to talking …\n6 \"She’s gradually being pushed out of parliament by her own people.Won’t be long &am…\n7 \"#AndrewBrigden for #PM in an English parliament #VoidTheUnion https://t.co/SNiAqQ6…\n8 \"Dummy parliament dummy speaker dummy pm. We not want any resolution against suprem…\n9 \"@Rendel_Harris @MikeyCycling @markandcharlie @AndyCoxDCS I think you need to ready…\n10 \"Attended Prize Distribution Ceremony of #MPs #CarRally on #RoadSafety organised by…\nFirst off I’ll declare the textdata library, define the sentiments I’m interested in and create a function with the sole purpose of standardising my chosen sentiments in the extracted data. In short, negative sentiments will need to be displayed as less than zero on my plot.\nlibrary(textdata)\n\nrequired_sentiments <- c(\"positive\",\"negative\")\n\noverall <- function(in_data){\n              mutated <- mutate(in_data,\n                               overall = (positive - negative),\n                               negative = (-1 * negative)\n                         )\n              pivot_longer(mutated,\n                           cols = required_sentiments,\n                           names_to = \"sentiment\",\n                           values_to = \"n\"\n           )\n}"
  },
  {
    "objectID": "posts/mastering-sentiment-analysis-of-twitter-data-with-r/index.html#the-loughran-mcdonald-dictionary",
    "href": "posts/mastering-sentiment-analysis-of-twitter-data-with-r/index.html#the-loughran-mcdonald-dictionary",
    "title": "Sentiment Analysis Of Twitter Data With R",
    "section": "1. The Loughran McDonald dictionary",
    "text": "1. The Loughran McDonald dictionary\nYou can find documentation for the Loughran McDonald dictionary here.\nNext it’s time to create the tokens, remove stop words and merge the sentiments from our dictionary with inner_join. Sentiment analysis depends wholly on categorising words with sentiment values.\n\nIn this step I also filter on my chosen sentiments and summarise the frequencies of each before transposing.\n\ntwtr_text <- as.data.frame(twtr) %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words) %>%\n  arrange(word) %>%\n  inner_join(get_sentiments(\"loughran\"), multiple = \"all\") %>%\n  filter(sentiment %in% required_sentiments)  %>%\n  count(word, sentiment) %>%\n  group_by(sentiment) %>%\n  summarise(sentiment2 = sum(n)) %>%\n  spread(sentiment, sentiment2) %>%\n\n  overall() \nThe result of merging the dictionary is below. Each token/word has been assigned a sentiment value for each row it appears in. Better yet, only rows that have matching words in the dictionary remain so we’ve fewer rows of data to handle.\n> print(twtr_text[1:10, c(\"id_str\",\"word\",\"sentiment\")])\n\n                \n   id_str              word    sentiment\n1  1640175617973841920   abide constraining\n2  1640191141839511553 abiding constraining\n3  1640676248849375232 abolish     negative\n4  1640218957368946690 abolish     negative\n5  1640209920950927360 abolish     negative\n6  1640176958930599938 abolish     negative\n7  1640207450778517504  abrupt     negative\n8  1640231567590309889 absence     negative\n9  1640219874650906625 absence     negative\n10 1640209883617263616 absence     negative"
  },
  {
    "objectID": "posts/mastering-sentiment-analysis-of-twitter-data-with-r/index.html#plotting-the-sentiment-values",
    "href": "posts/mastering-sentiment-analysis-of-twitter-data-with-r/index.html#plotting-the-sentiment-values",
    "title": "Sentiment Analysis Of Twitter Data With R",
    "section": "2. Plotting the sentiment values",
    "text": "2. Plotting the sentiment values\nAfter the frequency count and filtering the data looks much better. I have the sentiments I’m interested in and a frequency count of each so that this data is ready to be plotted.\n> print(twtr_text)\n\n# A tibble: 2 × 3\n  \noverall sentiment     n\n    <int> <chr>     <dbl>\n1   -1398 positive    205\n2   -1398 negative  -1603\nAnd that’s next…\nggplot(twtr_text,\n aes(x = sentiment, y = n, fill = sentiment)) +\n     geom_col(show.legend = TRUE) +\n     # facet_wrap(~ sentiment, scales = \"free\") +\n     coord_flip() +\n     labs(\n          title = \"Sentiment Analysis (Word Count)\",\n          x = \"Sentiment\",\n          y = \"Frequency\"\n  )\n\n\n\nUnsurprisingly, overwhelmingly negative.\n\n\nOh dear. Parliaments of the world, Twitter users say you need to step up."
  },
  {
    "objectID": "posts/mastering-sentiment-analysis-of-twitter-data-with-r/index.html#conclusion",
    "href": "posts/mastering-sentiment-analysis-of-twitter-data-with-r/index.html#conclusion",
    "title": "Sentiment Analysis Of Twitter Data With R",
    "section": "3. Conclusion",
    "text": "3. Conclusion\nSentiment analysis of Twitter data using R is so simple and yet so few brands use this as a technique for measuring their customer’s attitude. Perhaps brands don’t care or don’t have a big enough Twitter following to think this is worthwhile. However, it is a useful tool to help with growing Twitter influence and seeing the results can inspire change in attitude. For these two points alone it is one tool I always recommend clients use on a regular basis where appropriate.\n\n\n\n\n\n\nTip\n\n\n\nIf you’ve been affected by any of the topics mentioned in this article and would like a detailed sentiment analysis of your brand:- contact me."
  },
  {
    "objectID": "about.html#education-experience",
    "href": "about.html#education-experience",
    "title": "About Me",
    "section": "Education & Experience",
    "text": "Education & Experience\nMSc in Psychology Research Methods and many years experience in biostatistics. SAS programming, R programming, Python, C#, Ruby.\nKlaviyo Partner and MailChimp Certified.\nOn this blog I record what I learn so I can share it.\nYou can find me on Twitter and LinkedIn.\nSend me an email."
  },
  {
    "objectID": "posts/twitter-analysis-of-retweets-with-r/index.html",
    "href": "posts/twitter-analysis-of-retweets-with-r/index.html",
    "title": "Viewing Twitter Influence With Network Graphs",
    "section": "",
    "text": "Have you wondered how information spreads on Twitter?\nNetwork analysis and visualisation are powerful tools that allow us to understand the structure of social relationships and quantify internet communities.\nIn network analysis of internet communities such as Twitter we gain a better understanding of social relationships and information flow between users. Retweets, for example, can be used to identify key influencers and measure the spread of information within a community.\nA network graph is a visual representation of the flow of information between network users.\n\n\nTwo key terms to understand are ‘Vertices’ and ‘Edges’.\n\nVertices\n\nAlso called ‘nodes’. In the example that follows, a vertex is a Twitter user.\n\nEdges\n\nA pair relationship between two vertices. In this example our edges show who is being retweeted and by whom."
  },
  {
    "objectID": "posts/twitter-analysis-of-retweets-with-r/index.html#quarto",
    "href": "posts/twitter-analysis-of-retweets-with-r/index.html#quarto",
    "title": "Viewing Twitter Influence With Network Graphs",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "posts/twitter-analysis-of-retweets-with-r/index.html#running-code",
    "href": "posts/twitter-analysis-of-retweets-with-r/index.html#running-code",
    "title": "Viewing Twitter Influence With Network Graphs",
    "section": "Running Code",
    "text": "Running Code\nI’m going to demonstrate creating a network graph using the Twitter hashtag #MushroomMonday. It’s a fine tag to follow. One of the finest. Lots of photos of fungi in all their splendour. And yes, fungi can be very varied just like the Twitter accounts that post mushroom content.\n\nI’ll show that early on in the visualising process my graphs are usually messy. I think that’s normal because it’s normal for me. And because I’ve already decided I will eliminate some vertices then colour and resize the remainder, it’s a fairly straightforward approach.\nMy method here is not the most straightforward. I use it to get a grasp on the network and the data in it. As ever, there’s more than one way to achieve a goal."
  },
  {
    "objectID": "posts/twitter-analysis-of-retweets-with-r/index.html#what-is-a-network-graph",
    "href": "posts/twitter-analysis-of-retweets-with-r/index.html#what-is-a-network-graph",
    "title": "Viewing Twitter Influence With Network Graphs",
    "section": "What is a Network Graph?",
    "text": "What is a Network Graph?\nHave you wondered how information spreads on Twitter?\nNetwork analysis and visualisation are powerful tools that allow us to understand the structure of social relationships and quantify internet communities.\nIn network analysis of internet communities such as Twitter we gain a better understanding of social relationships and information flow between users. Retweets, for example, can be used to identify key influencers and measure the spread of information within a community.\nA network graph is a visual representation of the flow of information between network users.\n\nKey terminology\nTwo key terms to understand are ‘Vertices’ and ‘Edges’.\n\nVertices\n\nAlso called ‘nodes’. In the example that follows, a vertex is a Twitter user.\n\nEdges\n\nA pair relationship between two vertices. In this example our edges show who is being retweeted and by whom."
  },
  {
    "objectID": "posts/twitter-analysis-of-retweets-with-r/index.html#the-setup.",
    "href": "posts/twitter-analysis-of-retweets-with-r/index.html#the-setup.",
    "title": "Viewing Twitter Influence With Network Graphs",
    "section": "1. The setup.",
    "text": "1. The setup.\nObligatory libraries…\n\nlibrary(rtweet)\nlibrary(igraph)\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "posts/twitter-analysis-of-retweets-with-r/index.html#the-setup",
    "href": "posts/twitter-analysis-of-retweets-with-r/index.html#the-setup",
    "title": "Viewing Twitter Influence With Network Graphs",
    "section": "1. The setup",
    "text": "1. The setup\nObligatory libraries…\n\nlibrary(rtweet)\nlibrary(igraph)\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\n\nThe very first thing to do is authorise rtweet to fetch tweets for you. The Twitter API is now available at a cost, even if you only wish to read tweets. I know, I know, but what can we do?\n\n# The Twitter API is no longer allowing free access to read tweets. \n# If you've a paid up developer account you can use the search_tweets function like so...\n#\n# twts &lt;- search_tweets(\"#MushroomMonday\", n = 1000, include_rts = TRUE)\n#\n# Otherwise you'll need to count on a file of graph you've saved previously.\n# Like I did.\n\ngml &lt;- read_graph(\"./mushroommonday.graphml\", format = \"graphml\")\n\nLet’s check the output.\nA quick look at the edges…\n&gt; edge_attr(gml)\n$type\n  [1] \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\"\n [13] \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \nand vertices…\n&gt; vertex_attr(gml)\n$id\n  [1] \"72819944\"  \"1447502319944454148\" \"1430641982880239617\" \"36879139\"            \"1506195335579938818\"\n  [6] \"1407036811151556613\" \"2354121475\"  \"2216026806\"  \"1450456500418367490\" \"16872011\"           \n\n$name\n  [1] \"bernoid\"  \"ada_apocalypse\"  \"ThatFellaCrafty\" \"marypele\"        \"Cottage_C_Craft\"  \"LizLouise12\"    \n  [7] \"coachmegjade\"  \"PacificRimNPR\"  \"LukesMushrooms\"  \"LGTrombatore\"  \"fun_g4l\"         \"Whatknotsartist\"\n [13] \"LX8111\"  \"MacroMarie\"  \"GreatLakesFungi\" \"TanukisRevenge\"  \"muscariamadge\"   \"ray_rambling\"   \n [19] \"TheSuffolkMan\"  \"keeper_of_books\" \"BobJoostens\"  \"TheWitchersOne\"  \"\n \nNot a lot to see so I’ll try a quick plot.\nset.seed(127926)\nplot(gml,  layout = layout.fruchterman.reingold)\nHmmm. Not pretty.\n\n\n\nPoor effort. Needs work.\n\n\nThe plot is crowded and I can’t read the labels. The vertices have a name attribute which is the user’s Twitter handle. Every vertex is the same size and colour.\nIt doesn’t convey any information at all.\nIt’s useless.\nThe connections between the vertices are barely visible. These faint lines are the edges.\nThey have only one attribute, type, which shows if the tweet was a mention, reply or retweet.\nI’ll start by filtering out the replies and mentions. Retweets only beyond this point.\n# Keep only retweets\ngml &lt;- gml - E(gml)[E(gml)$type == \"retweet\"]\nAnd locate the vertices with the most edges by assigning each edge a weight and summing the weights for each vertex.\n# Find out who has the most edges\n## Set a default edge weight\nE(gml)$weight &lt;- 1\n\n## Calculate the strength of each vertex\nV(gml)$strength &lt;- strength(\n  gml,\n  vids = V(gml),\n  mode = \"total\",\n  loops = TRUE,\n  weights = NULL\n)\nUsers that have had tweets retweeted the most have the highest values for strength.\n# &gt; V(gml)$strength\n# [1] 54  9 10 36  2  2  3  2  8  2 83  2  6  2 63 46 16 18 49 34  1  2  8  2  1 12  2  2  6  2 22  2  6  3  2 21  2 12 12 58  4 20 12 16 10  4  7\n&gt; hist(V(gml)$strength)\n\n\n\n\n\nThere’s quite a mixture of strength values here and I’m only interested in the strongest nodes.\n# Remove vertices that do not meet the mean strength value\n#  - Create a subgraph of vertices that don't meet threshold strength\ngmls &lt;- subgraph(gml, which(V(gml)$strength &gt;= mean(V(gml)$strength)))\n\n# remove isolated vertices (with no edges)\nisolated = which(degree(gmls, mode = \"total\")==0)\ngmls = delete.vertices(gmls, isolated)\nAnd I can keep labels only for those who are the highest performing vertices. It may mean that there are some unlabelled nodes but that shouldn’t be a problem because they’re not important.\n# Keep labels only for vertices with the highest strength\nV(gmls)$label &lt;- ifelse(V(gmls)$strength &gt; 14, V(gmls)$name, NA)\nIt would be useful to sneak a quick peek at what I’m left with. I’m using a force-directed layout and I’d like to have some space between vertices. First things first, I’ll reset the edge weights to a very small value. Since the edge weights are the force responsible for holding the vertices together, a smaller weight should give me what I’m after.\n# Reset edge weights to spread out the graph\nE(gmls)$weight &lt;- E(gmls)$weight/100000\n\n# Plot for another look-see\nset.seed(127926)\nplot(gmls,\n     layout = layout.fruchterman.reingold, \n     vertex.size = 5,  \n     edge.arrow.size = 0.25)\n\n\n\nGetting there.\n\n\nI can work with this. It’s time to start making the plot presentable. I’ll change the vertex size as a function of strength. The higher the vertex’s strength, the larger it will be.\nColour is also vital now. I pick a palette using the RColorBrewer library and choose one that I think is close to the colours of Turkey Tail mushrooms. I think the folks in #MushroomMonday would like it.\n# Now we're close. Let's change the vertex size depending on strength and add colour.\n# Vertex size will indicate the amount of retweets by other users, it will be a function of strength.\n\nlibrary(RColorBrewer)\n\ncol &lt;- brewer.pal(5, \"RdPu\")\nfor(v in V(gmls)){\n  # Colour from light to dark\n  if (V(gmls)$strength[v] &lt; 15){\n    V(gmls)$s_class[v] &lt;- 3\n  } else if (V(gmls)$strength[v] &lt; 30){\n    V(gmls)$s_class[v] &lt;- 4\n  } else\n    V(gmls)$s_class[v] &lt;- 5\n}\nset.seed(127926)\n\nplot(gmls,\n     layout = layout.fruchterman.reingold, \n     vertex.label = V(gmls)$label, \n     vertex.label.color = col[V(gmls)$s_class],\n     vertex.label.cex=0.8,\n     vertex.label.dist = -1.6,\n     vertex.label.degree = -pi/2,\n     vertex.size = sqrt(V(gmls)$strength)*2.4,\n     vertex.color = col[V(gmls)$s_class],\n     vertex.frame.color = col[1],\n     edge.curved = 0.3,\n     edge.arrow.size = 0.25,\n     edge.color = col[2])\n\nOK, so it more ‘fairy princess’ than Turkey Tail but it tells a story. The users who are retweeted more than most are are the darkest purple, while the weakest nodes - those with lower values of `strength` are smaller, pinker and unlabelled.\nIf I wanted my tweets to be noticed by others I’d hope that they were retweeted by one of the stronger accounts.\nTrouble is that hope really doesn’t get you anywhere.\nSo how can you increase your chances of being noticed? You could follow all of the accounts, sure. But Twitter penalises tweets from account with a high following:follower ratio, so you need to be selective. Which accounts will get you the biggest return on investment then?\nLet’s look at the communities.\nUsers in the same cluster, or clique, are more connected with one another that users outside of the cluster. This will tell me if there are divisions due to different types of account. Put another way in our fungi example, there may be accounts that prefer scientific data and accounts that prefer photographic content. This info show up in separate clusters.\nA small tweak to my graph and I notice that there is a split between the interesting accounts.\nTake a look for yourself below. The nodes are now coloured according to the clusters they belong to. Green, orange and blue nodes are in different clusters. Our strong accounts are now split three ways. I would go on to check the content of these communities and determine which accounts are more suited for my client."
  },
  {
    "objectID": "posts/twitter-analysis-of-retweets-with-r/index.html#in-degree-and-out-degree",
    "href": "posts/twitter-analysis-of-retweets-with-r/index.html#in-degree-and-out-degree",
    "title": "Viewing Twitter Influence With Network Graphs",
    "section": "In-degree and out-degree",
    "text": "In-degree and out-degree\n\nexplain these terms\n\nI’ll start simplifying this graph by keeping only vertices that really matter. I’ll create a subgraph using only the vertices that have a high in-degree.\nLet’s calculate in-degree and add it as an attribute to each of the nodes.\n\n# Calculate in-degree\nV(gml)$indegree = degree(gml,mode=\"in\") \n\nUse the mean of in-degree as the threshold…\n\n# Set a reasonable threshold for in-degree\ncut_off_in &lt;- mean(V(gml)$indegree)\nthreshold_in &lt;- which(V(gml)$indegree &gt; cut_off_in)\n\nAnd create a subgraph from our list of vertices that exceed that threshold.\n\n#create a subgraph of vertices that exceed threshold in-degree\ngmls &lt;- subgraph(gml, threshold_in)\n\nI also need to spread out all the overlapping vertices. This is quite straightforward to do when using the edge weight\n\nE(gmls)$weight &lt;- 0.00001\n\nA quick plot? Oh, go on then. Won’t hurt.\nLet’s use a pre-defined layout, remove the vertex labels and set the vertex size to be small. Arrows can be smaller too.\n# Plot \nset.seed(127926)\nplot(gmls,\n    layout = layout.fruchterman.reingold, \n     vertex.size = 1,\n     vertex.label = NA,\n     edge.arrow.size = 0.25\n      )\n\n\n\nBetter. Still room for improvement.\n\n\nOptions for graph appearance are as numerous as ice-cream flavours. There’s so many. You’ll notice I’ve introduced a few above.\nMy graph is looking lean at last. I can simplify further though…\n# Simplify\ngmls &lt;- igraph::simplify(gmls, remove.multiple = F, remove.loops = T)\nYou know another way that graph options are like ice-cream? Colours. All the colours we need. Let’s get some.\n\nlibrary(RColorBrewer)\n\ncol &lt;- brewer.pal(5, \"RdPu\")"
  },
  {
    "objectID": "posts/twitter-analysis-of-retweets-with-r/index.html#twitter-retweet-network-example",
    "href": "posts/twitter-analysis-of-retweets-with-r/index.html#twitter-retweet-network-example",
    "title": "Viewing Twitter Influence With Network Graphs",
    "section": "Twitter retweet network example",
    "text": "Twitter retweet network example\nBefore I visualise a network I consider the context and requirements. For example, do I want to:\n\nAssign colors to vertices based on some node attributes?\nSet the vertex size as a function of some attributes?\nShow all vertices?\n\nDeciding these answers early on saves time later.\nI’m going to demonstrate creating a network graph using the Twitter hashtag #MushroomMonday. It’s a fine tag to follow. One of the finest. Lots of photos of fungi in all their splendour. And yes, fungi can be very varied just like the Twitter accounts that post mushroom content.\nI’ll show that early on in the visualising process my graphs are usually messy. I think that’s normal because it’s normal for me. And because I’ve already decided I will eliminate some vertices then colour and resize the remainder, it’s a fairly straightforward approach.\nMy method here is not the most straightforward. I use it to get a grasp on the network and the data in it. As ever, there’s more than one way to achieve a goal.\n\n1. The setup\nObligatory libraries…\n\nlibrary(rtweet)\nlibrary(igraph)\n\nThe very first thing to do is authorise rtweet to fetch tweets for you. The Twitter API is now available at a cost even if you only wish to read tweets. I know, I know, but what can we do?\nLuckily for me I saved the data for this particular graph earlier1. I load it here.\n\n# The Twitter API is no longer allowing free access to read tweets. \n# If you've a paid up developer account you can use the search_tweets function like so...\n#\n# twts &lt;- search_tweets(\"#MushroomMonday\", n = 1000, include_rts = TRUE)\n#\n# Otherwise you'll need to count on a file of graph you've saved previously.\n# Like I did.\n\ngml &lt;- read_graph(\"./mushroommonday.graphml\", format = \"graphml\")\n\nLet’s check the output.\nA quick look at the edges…\n&gt; edge_attr(gml)\n$type\n  [1] \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\"\n [13] \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \"mention\" \nand vertices…\n&gt; vertex_attr(gml)\n$id\n  [1] \"72819944\"  \"1447502319944454148\" \"1430641982880239617\" \"36879139\"            \"1506195335579938818\"\n  [6] \"1407036811151556613\" \"2354121475\"  \"2216026806\"  \"1450456500418367490\" \"16872011\"           \n\n$name\n  [1] \"bernoid\"  \"ada_apocalypse\"  \"ThatFellaCrafty\" \"marypele\"        \"Cottage_C_Craft\"  \"LizLouise12\"    \n  [7] \"coachmegjade\"  \"PacificRimNPR\"  \"LukesMushrooms\"  \"LGTrombatore\"  \"fun_g4l\"         \"Whatknotsartist\"\n [13] \"LX8111\"  \"MacroMarie\"  \"GreatLakesFungi\" \"TanukisRevenge\"  \"muscariamadge\"   \"ray_rambling\"   \n [19] \"TheSuffolkMan\"  \"keeper_of_books\" \"BobJoostens\"  \"TheWitchersOne\"  \"\n \nNot a lot to see so I’ll try a quick plot.\n\n\n2. First plot is the messiest. Always.\nset.seed(127926)\nplot(gml,  layout = layout.fruchterman.reingold)\nHmmm. Not pretty.\n\n\n\nPoor effort. Needs work.\n\n\nThe plot is crowded and I can’t read the labels. The vertices have a name attribute which is the user’s Twitter handle. Every vertex is the same size and colour.\nIt doesn’t convey any information at all.\nIt’s useless.\nThe connections between the vertices are barely visible. These faint lines are the edges. They have only one attribute, type, which shows if the tweet was a mention, reply or retweet.\nI’ll change all that now.\nI’ll start by filtering out the replies and mentions. Retweets only beyond this point.\n# Keep only retweets\ngml &lt;- gml - E(gml)[E(gml)$type == \"retweet\"]\nAnd locate the vertices with the most edges by assigning each edge a weight and summing the weights for each vertex.\n# Find out who has the most edges\n## Set a default edge weight\nE(gml)$weight &lt;- 1\n\n## Calculate the strength of each vertex\nV(gml)$strength &lt;- strength(\n  gml,\n  vids = V(gml),\n  mode = \"total\",\n  loops = TRUE,\n  weights = NULL\n)\nUsers that have had tweets retweeted the most have the highest values for strength.\n# &gt; V(gml)$strength\n# [1] 54  9 10 36  2  2  3  2  8  2 83  2  6  2 63 46 16 18 49 34  1  2  8  2  1 12  2  2  6  2 22  2  6  3  2 21  2 12 12 58  4 20 12 16 10  4  7\n&gt; hist(V(gml)$strength)\n\n\n\n\n\nThere’s quite a mixture of strength values here and I’m only interested in the strongest nodes.\n# Remove vertices that do not meet the mean strength value\n#  - Create a subgraph of vertices that don't meet threshold strength\ngmls &lt;- subgraph(gml, which(V(gml)$strength &gt;= mean(V(gml)$strength)))\n\n# remove isolated vertices (with no edges)\nisolated = which(degree(gmls, mode = \"total\")==0)\ngmls = delete.vertices(gmls, isolated)\nAnd I can keep labels only for those who are the highest performing vertices. It may mean that there are some unlabelled nodes but that shouldn’t be a problem because they’re not important.\n# Keep labels only for vertices with the highest strength\nV(gmls)$label &lt;- ifelse(V(gmls)$strength &gt; 14, V(gmls)$name, NA)\nIt would be useful to sneak a quick peek at what I’m left with. I’m using a force-directed layout and I’d like to have some space between vertices. First things first, I’ll reset the edge weights to a very small value. Since the edge weights are the force responsible for holding the vertices together, a smaller weight should give me what I’m after.\n# Reset edge weights to spread out the graph\nE(gmls)$weight &lt;- E(gmls)$weight/100000\n\n# Plot for another look-see\nset.seed(127926)\nplot(gmls,\n     layout = layout.fruchterman.reingold, \n     vertex.size = 5,  \n     edge.arrow.size = 0.25)\n\n\n\nGetting there.\n\n\n\n\n3. Changing vertex size and colour\nI can work with the graph above. It’s time to start making the plot presentable.\nI’ll change the vertex size as a function of strength. The higher the vertex’s strength, the larger it will be.\nColour is also vital now. I pick a palette using the RColorBrewer library and choose one that I think is close to the colours of Turkey Tail mushrooms. I think the folks in #MushroomMonday would like it.\n# Now we're close. Let's change the vertex size depending on strength and add colour.\n# Vertex size will indicate the amount of retweets by other users, it will be a function of strength.\n\nlibrary(RColorBrewer)\n\ncol &lt;- brewer.pal(5, \"RdPu\")\nfor(v in V(gmls)){\n  # Colour from light to dark\n  if (V(gmls)$strength[v] &lt; 15){\n    V(gmls)$s_class[v] &lt;- 3\n  } else if (V(gmls)$strength[v] &lt; 30){\n    V(gmls)$s_class[v] &lt;- 4\n  } else\n    V(gmls)$s_class[v] &lt;- 5\n}\nset.seed(127926)\n\nplot(gmls,\n     layout = layout.fruchterman.reingold, \n     vertex.label = V(gmls)$label, \n     vertex.label.color = col[V(gmls)$s_class],\n     vertex.label.cex=0.8,\n     vertex.label.dist = -1.6,\n     vertex.label.degree = -pi/2,\n     vertex.size = sqrt(V(gmls)$strength)*2.4,\n     vertex.color = col[V(gmls)$s_class],\n     vertex.frame.color = col[1],\n     edge.curved = 0.3,\n     edge.arrow.size = 0.25,\n     edge.color = col[2])\n\nOK, so it more ‘fairy princess’ than Turkey Tail but it tells a story. The users who are retweeted more than most are are the darkest purple, while the weakest nodes - those with lower values of `strength` are smaller, pinker and unlabelled.\nIf I wanted my tweets to be noticed by others I’d hope that they were retweeted by one of the stronger accounts.\nTrouble is that hope really doesn’t get you anywhere.\n\n\n4. Visualising cliques and communities\nSo how can you increase your chances of being noticed? You could follow all of the accounts, sure. But Twitter penalises tweets from accounts with a high following:follower ratio, so you need to be selective. Which accounts will get you the biggest return on investment then?\nLet’s look at the communities.\nUsers in the same cluster, or clique, are more connected with one another that users outside of the cluster. This will tell me if there are divisions due to different types of account. Put another way in our fungi example, there may be accounts that prefer scientific data and accounts that prefer photographic content. This info shows up in separate clusters.\nA small tweak to my graph and I notice that there is a split between the interesting accounts.\nTake a look for yourself below. The nodes are now coloured according to the clusters they belong to. Green, orange and blue nodes are in different clusters. Our strong accounts are now split three ways. I would go on to check the content of these communities and determine which accounts are more suited for my client.\n\n\n\n5. Conclusion\n\na. What improvements would I make to the code?\nGraphs that are more cluttered often benefit from an interactive visualisation or application. This isn’t one of them. This is such a simple example I probably won’t make any adjustments of note.\nIn order to demonstrate the most basic concepts of network graphs I’ve steered clear of using coreness and plotting k-core. I’d normally use those myself instead of summing vertex strength but either approach works.\nYMMV.\n\n\nb. How should I interpret this graph for my client?\nOpinions are like.. well, never mind. Here’s mine. The graph informs us that:\n\nThere are multiple strong accounts that will be worth pursuing as influencers\nThose accounts are in different clusters\nThe cluster with content most similar to the client’s is likely to have followers with similar interests\nTo gain engagement, the followers of the cluster’s main account(s) should be followed to engage with our content\n\nAll in all this has been a worthwhile exercise to save tweeting into the void."
  },
  {
    "objectID": "posts/twitter-analysis-of-retweets-with-r/index.html#footnotes",
    "href": "posts/twitter-analysis-of-retweets-with-r/index.html#footnotes",
    "title": "Viewing Twitter Influence With Network Graphs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’d like example data to practice on you can download a dataset from Stanford University’s SNAP.↩︎"
  },
  {
    "objectID": "about.html#mark-vanderstay",
    "href": "about.html#mark-vanderstay",
    "title": "About Me",
    "section": "",
    "text": "I’m a email marketer with a background in statistical programming and a passion for R, data visualization, Shiny and machine learning."
  }
]