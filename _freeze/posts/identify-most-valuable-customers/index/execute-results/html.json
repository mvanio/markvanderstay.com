{
  "hash": "2f235af1f8fa88ffee0e7eedb79baa82",
  "result": {
    "markdown": "---\ntitle: \"Making the most of customer relationships with CLV\"\nformat: \n  html:\n    fig-width: 8\n    fig-height: 6\neditor: visual\ndate: \"2023-04-17\"\ncategories: [CLV, R]\nwebsite:\n  open-graph: true\n  image: \"https://markvanderstay.com/posts/identify-most-valuable-customers/predicting-future-spending.jpg\"  \n  twitter-card: \n    description: \"Just as a crystal ball can reveal glimpses of the future, CLV can predict a customer's future spending habits. By using predictive analytics, businesses can estimate the value of a customer's future purchases based on their past behaviour. Armed with this knowledge, businesses can create targeted marketing campaigns to keep these valuable customers coming back for more.\"\n    image: \"https://markvanderstay.com/posts/identify-most-valuable-customers/predicting-future-spending.jpg\"  \nslug: \"identify-most-valuable-customers-with-clv\"\n---\n\n![CLV leads businesses to the most valuable customers, those who will continue to spend money with them for years to come. By understanding how much these customers are worth over time, businesses can create strategies to keep them happy and engaged, and prevent them from jumping ship to the competition.](./predict-future-spending.png)\n\n\n\n\n\n## Introduction\n\n#### Making the most of customer relationships with CLV\n\nAs a data scientist working in the e-commerce industry, you possess the unique ability to turn raw data into meaningful insights that can drive business growth. One area where your expertise can have a significant impact is in understanding and predicting Customer Lifetime Value (CLV).\n\n**In today's fast-paced and competitive e-commerce landscape, developing strategies that make the most of customer relationships is crucial to long-term success. By harnessing the power of CLV, you can help businesses better target their marketing efforts, retain loyal customers, and ultimately increase overall revenue.**\n\nIn this blog post, I will guide you step-by-step through the process of measuring and predicting CLV, ensuring you have the tools and knowledge needed to become a valuable asset in the e-commerce industry. Our approach will be informative, practical, and designed to cater to your analytical mindset, providing you with a comprehensive understanding of the techniques and methodologies involved in CLV analysis.\n\nJoin me on this journey into the realm of customer loyalty, as I equip you with the skills needed to master CLV and contribute to the sustainable growth and success of any e-commerce business.\n\n## 1. In the beginning...\n\nAs ever, the best way to start is to install the necessary packages. `CLVTools`[^1] is where it's at. You know the drill.\n\n[^1]: https://cran.r-project.org/web/packages/CLVTools/index.html\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(CLVTools)\n```\n:::\n\n\nThe next step is to load the transaction data which has been formatted to show a purchase date and customer ID in every row. At this point a transaction cost isn't necessary but I have yet to find a store owner who provided transaction data without price values. Price is optional here. However, when provided, this variable will allow prediction of future spending. I'll cover that here in this post when I delve into the predictive side of CLV.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransactions <- read.csv(\"./data/transactions.csv\")\n\nhead(transactions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Id       Date  Price\n1  1 2017-01-12  20.86\n2  2 2016-12-31  51.18\n3  2 2016-12-31  44.00\n4  3 2016-12-31 102.87\n5  4 2017-01-12  43.80\n6  4 2017-02-08  10.50\n```\n:::\n:::\n\n\nAs you can see, there's one observation per transaction and customer ID will be repeated for multiple transactions.\n\n## 2. Create a `clv.data` object\n\nCreating a Customer Lifetime Value (CLV) data object is straightforward but it helps to know a little of the intricacies. The process involves splitting the transaction data into estimation and holdout samples using the `estimation.split` parameter.\n\nIn the example below I've set the `estimation.split` to be 39 weeks. The first 39 weeks of data will be used to create the estimation sample while the remaining data becomes the holdout sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a CLV data object, split data into estimation and holdout samples\nclv.trans <- clvdata(data.transactions = transactions, \n                     date.format = \"ymd\",\n                     time.unit = \"week\", \n                     estimation.split = 39, \n                     name.id = \"Id\")\n```\n:::\n\n\n::: callout-note\n## Estimation & Holdout\n\n1.  Estimation sample: This sample is used to estimate the parameters of the CLV model. It's the portion of data on which the model is trained and built.\n\n2.  Holdout sample: This sample is used to evaluate the performance and accuracy of the CLV model. It's the portion of data that the model has not seen during the estimation process and acts as a validation set.\n:::\n\nHow did I decide that 39 weeks is the right value to use? It's a 'best guess'. To decide on a good value for **`estimation.split`** means looking at the data, the business and the business goals. Here's a few factors to consider:\n\n1.  **Data availability:** The estimation.split value should be chosen such that there is sufficient data available for both the estimation and holdout samples. A larger dataset usually allows for a more robust model with more accurate validation.\n\n2.  **Seasonality and trends**: Is the business seasonal, with seasonal patterns or trends? If so, it's crucial to ensure that the estimation and holdout samples capture these variations. Doing so may require selecting an estimation.split value that includes multiple cycles of seasonality.\n\n3.  **Model stability**: A good `estimation.split` value should result in a model that is stable and reliable. I can assess this by comparing model performance across different split values and selecting the one that yields the most consistent and accurate results.\n\nWhile the algorithm can handle some inaccuracy when setting the `estimation.split` value, the more accurate it is to start with, the better the results will be. Of course, better results at this point will mean a more accurate model and improved decision-making and business outcomes. While there are methods to mitigate the effects of inaccurate `estimate.split` values, in my experience it's just good practice to get it right first time and avoid messing around with other metrics such as mean squared error, mean absolute error, or root mean squared error later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# summary of data\nsummary(clv.trans)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCLV Transaction Data \n                                \nTime unit         Weeks         \nEstimation length 39.0000 Weeks \nHoldout length    40.71429 Weeks\n\nTransaction Data Summary \n                                   Estimation      Holdout         Total     \nNumber of customers                -               -               263       \nFirst Transaction in period        2016-12-31      2017-10-01      2016-12-31\nLast Transaction in period         2017-09-30      2018-07-13      2018-07-13\nTotal # Transactions               1293            976             2269      \nMean # Transactions per cust       4.916           8.342           8.627     \n(SD)                               5.792           9.094           12.353    \nMean Spending per Transaction      39.713          38.989          39.401    \n(SD)                               42.253          58.977          50.124    \nTotal Spending                     51348.290       38053.580       89401.870 \nTotal # zero repeaters             87              -               -         \nPercentage of zero repeaters       33.080          -               -         \nMean Interpurchase time            7.308           5.447           9.403     \n(SD)                               6.767           5.432           12.197    \n```\n:::\n:::\n\n\nI've been provided with data for 263 customers and 2269 transactions. Just over 33% are `zero-repeaters` that is, customers who purchased once but have not returned since. The mean spending per transaction was ¬£39.71 in the estimation period and ¬£38.99 in the holdout period. Overall mean spending per transaction was ¬£39.40. I already have some valuable insights into customer behaviour and spending patterns but I can take this further. Much further.\n\n## 3. Fit a Pareto/NBD model\n\nNow I have my `clvdata()` object, I can estimate a model using the standard Pareto/NBD model. For this I will use the the `pnbd()` command and pass it the object created in the previous step as the first argument. Starting values for model parameters are provided using the arguments start.params.model with a vector of values. I find that this can be useful if prior knowledge on distribution parameters is available but I don't worry too much if I can't be accurate. While these initial values impact the convergence of the model fitting process, they are simply best guesses. in practice,\n\n::: callout-tip\nI find it can be challenging to obtain accurate starting values. The defaults often work well enough in many scenarios.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PNBD model fit on the first 39 periods\npnbd.trans <- pnbd(clv.trans,\n                   start.params.model = c(r=0.5, alpha=8, s=0.5, beta=10))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nStarting estimation...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nEstimation finished!\n```\n:::\n:::\n\n\n::: callout-note\nThe `start.params.model` parameters are as follows:\n\nr\n\n:   Shape parameter for the Gamma distribution of the transaction rate (ùõå)\n\nalpha\n\n:   Scale parameter for the Gamma distribution of the transaction rate (ùõå)\n\ns\n\n:   Shape parameter for the Gamma distribution of the lifetime (ùúá)\n\nbeta\n\n:   Scale parameter for the Gamma distribution of the lifetime (ùúá)\n\nThe values you and I provide for these parameters will serve as the starting point for the estimation process. The optimisation algorithm will iterate from these starting values to find the best-fitting parameters for the data so don't be too concerned about calculating them accurately beforehand.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the fitted model to the actual repeat transactions\nplot(pnbd.trans)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPlotting from 2016-12-31 until 2018-07-15.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n\nThe tracking plot is the default option when plotting the fitted model. It shows the repeated transactions by real customers against the models' predicted transactions.\n\nOverall it doesn't look too bad. There's a peak of repeated transactions at the end of 2017 and the beginning of 2018 but I suspect this could be due to the festive holiday and the related Christmas madness. It doesn't look unusual to me and I will mark it down as 'seasonality' and shrug it off after confirming with the business owner.\n\n## 4. Interpreting the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# inspect fit\nsummary(pnbd.trans)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPareto NBD Standard  Model \n\nCall:\npnbd(clv.data = clv.trans, start.params.model = c(r = 0.5, alpha = 8, \n    s = 0.5, beta = 10))\n\nFitting period:                               \nEstimation start  2016-12-31   \nEstimation end    2017-09-30   \nEstimation length 39.0000 Weeks\n\nCoefficients:\n      Estimate Std. Error z-val Pr(>|z|)    \nr       0.7513     0.1260 5.961 2.50e-09 ***\nalpha   5.2628     0.8871 5.932 2.99e-09 ***\ns       0.3730     0.1895 1.969    0.049 *  \nbeta   11.0325    10.0368 1.099    0.272    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nOptimization info:                 \nLL     -2828.5777\nAIC    5665.1553 \nBIC    5679.4439 \nKKT 1  TRUE      \nKKT 2  TRUE      \nfevals 17.0000   \nMethod L-BFGS-B  \n\nUsed Options:                 \nCorrelation FALSE\n```\n:::\n:::\n\n\nNow I see the `r`, `alpha`, `s` and `beta` that the algorithm arrived at. From these I can derive a couple of important pieces of info:\n\n-   Average purchase rate (ùëü/‚ç∫) = 0.143 transactions\n\n-   Average attrition rate (ùíî/ùû´) = 0.034 per customer per week\n\n-   Optimisation criteria were met. KKT1 & 2 are TRUE.\n\nSo that's nice. Let's go further still and to some more juicy, predictive analytics!\n\n## 5. Get predictive!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction.trans <- predict(pnbd.trans)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPredicting from 2017-10-01 until (incl.) 2018-07-13 (40.86 Weeks).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nEstimating gg model to predict spending...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStarting estimation...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nEstimation finished!\n```\n:::\n\n```{.r .cell-code}\nhead(prediction.trans)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Id period.first period.last period.length actual.x actual.total.spending\n1:    1   2017-10-01  2018-07-13      40.85714        0                  0.00\n2:   10   2017-10-01  2018-07-13      40.85714        0                  0.00\n3:  100   2017-10-01  2018-07-13      40.85714       23                750.27\n4: 1000   2017-10-01  2018-07-13      40.85714       24               1116.87\n5: 1001   2017-10-01  2018-07-13      40.85714       11                369.60\n6: 1002   2017-10-01  2018-07-13      40.85714        0                  0.00\n       PAlive        CET       DERT predicted.mean.spending predicted.CLV\n1: 0.36160395  0.2305102 0.05989818                40.25354      2.411114\n2: 0.04368167  0.1111880 0.02900386                35.55456      1.031220\n3: 0.94346385 12.3342954 3.22145319                46.68502    150.393616\n4: 0.98009916 12.6347184 3.27800310                42.40391    139.000145\n5: 0.54793815  3.9244903 1.01818583                46.15505     46.994418\n6: 0.42913180  1.3175031 0.34181840                37.16120     12.702380\n```\n:::\n:::\n\n\n::: callout-note\n\nCET\n\n:   Conditional Expected Transactions is the number of transactions to expect from a customer during the prediction period\n\nPAlive\n\n:   Probability of a customer being alive (active) at the end of the estimation period\n\nDERT\n\n:   Discounted Expected Residual Transactions is the total number of transactions for the remaining (residual) lifetime of a customer discounted to the end of the estimation period\n\nactual.predicted.mean.spending\n\n:   Predicted mean spending estimated by the Gamma/Gamma model\n\nactual.ùîÅ\n\n:   the number of actual transactions\n\nactual.total.spending\n\n:   the true spending amount\n\nCLV\n\n:   Calculated as the product of DERT and predicted spending\n:::\n\nAnd there we have it. A predicted value for the amount of money a customer is expected to spend over the course of their relationship with a business. Adding covariates to the predictive model could be a useful next step. I'll save that task for another post.\n\n## 6. Conclusion\n\nBy measuring CLV, the the analyst is able to help the business owner to identify their most valuable customers and implement targeted marketing campaigns to retain customer loyalty and increase overall revenue. Owners may also discover that it is more cost-effective to retain existing customers than to acquire new ones, which leads to a shift in their marketing strategy. By measuring CLV, the business owner is able to make more informed decisions that positively impact their bottom line\n\nMeasuring CLV should be a critical component of any e-commerce business' marketing strategy. However, many small businesses neglect this simple step and fail to segment their marketing accordingly. By understanding the value of their customers over time, businesses can better target their marketing efforts, retain loyal customers, and increase overall revenue. CLV analysis is a powerful tool that can help businesses make more informed decisions and stay competitive in a crowded marketplace.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}